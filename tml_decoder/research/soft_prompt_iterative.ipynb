{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dominik/Desktop/TML/TMLenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/dominik/Desktop/TML/TMLenv/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "text = \"\"\"Positional control of pneumatic manipulators for construction tasks\n",
    "This paper describes solutions that can be applied to pneumatic manipulator\n",
    "\tproblems in positioning, both for angle trajectories and for long\n",
    "\tlinear trajectories, used in construction tasks. Optimal positioning of\n",
    "\ta pneumatic manipulator along angle trajectories with minimum control\n",
    "\tenergy consumption is given. The implementation of the control system\n",
    "\tis presented. Control algorithms for a long linear trajectory\n",
    "\tmanipulator based on two-phase and three-phase motion modes of the\n",
    "\tend-effector are investigated. Conventional and fuzzy logic controls of\n",
    "\ta pneumatic manipulator were applied and experimental testing was\n",
    "\tcarried out. The obtained results allow widening the application range\n",
    "\tof pneumatic manipulators in construction, particularly in gantry type\n",
    "\tmachines\"\"\"\n",
    "\n",
    "enc_tokenizer = encoder.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 50257])\n",
      "torch.Size([1, 9])\n",
      " chocolate\n",
      "next token in the sentence is:  chocolate\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "generator = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "example_text = \"I really like eating ice cream and drinking hot\"\n",
    "inputs = gpt_tokenizer(example_text, return_tensors=\"pt\")\n",
    "outputs = generator(**inputs, labels=inputs[\"input_ids\"])\n",
    "logits = outputs.logits\n",
    "print(logits.shape)\n",
    "am_tokens = torch.argmax(logits, dim=-1)\n",
    "print(am_tokens.shape)\n",
    "decoded_tokens = am_tokens[0].tolist()\n",
    "last_token = decoded_tokens[-1]\n",
    "decoded_token = gpt_tokenizer.decode(last_token)\n",
    "print(decoded_token)\n",
    "print(\"next token in the sentence is:\", decoded_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded input: {'input_ids': tensor([[  40, 1107,  588, 6600, 4771, 8566,  290, 7722, 3024]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[' chocolate', ' coffee', ' water']\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_top_k_next_tokens(text, k=1):\n",
    "    assert k > 0 and k <= len(gpt_tokenizer.vocab)\n",
    "    # Tokenize the input text\n",
    "    encoded_input = gpt_tokenizer(text, return_tensors='pt')\n",
    "    print(\"encoded input:\", encoded_input)\n",
    "    # Get model output\n",
    "    output = generator(**encoded_input)\n",
    "    logits = output.logits\n",
    "    probabilities = F.softmax(logits, dim=-1)[0][-1]\n",
    "\n",
    "    # Get top k indices for each position in the sequence\n",
    "    top_k_indices = torch.topk(probabilities, k, dim=-1).indices\n",
    "    # Decode each index using the tokenizer\n",
    "    decoded_tokens = [gpt_tokenizer.decode(int(idx)) for idx in top_k_indices]\n",
    "\n",
    "    return decoded_tokens\n",
    "\n",
    "# Specify the number of top-k tokens to retrieve\n",
    "k = 3\n",
    "result = get_top_k_next_tokens(example_text, k)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "<class 'torch.nn.modules.sparse.Embedding'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54559/4090252663.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0599682554602623 117153.96875\n"
     ]
    }
   ],
   "source": [
    "unused_token = next(\n",
    "        k for k, v in enc_tokenizer.vocab.items()\n",
    "        if \"[unused\" in k)\n",
    "unused_token_id = enc_tokenizer.vocab[unused_token]\n",
    "new_token_id = unused_token_id\n",
    "non_new_token_ids = torch.tensor([i for i in range(len(enc_tokenizer.vocab)) if i != new_token_id])\n",
    "\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "print(len(enc_tokenizer))\n",
    "print(type(encoder[0].auto_model.embeddings.word_embeddings))\n",
    "\n",
    "dot_prods = []\n",
    "\n",
    "for idx, p in enumerate(encoder[0].auto_model.embeddings.word_embeddings.parameters()):\n",
    "    p.requires_grad = True\n",
    "    t = torch.tensor(p)\n",
    "    for x in range(t.shape[0]):\n",
    "        subt = t[x]\n",
    "        dot_prods.append(torch.dot(subt, subt).item())\n",
    "\n",
    "print(min(dot_prods), max(dot_prods))\n",
    "\n",
    "assert any(p.requires_grad for p in encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09973382949829102"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "def soft_prompt_for_text(text, target):\n",
    "    tokenized_text = enc_tokenizer.encode(text)\n",
    "    tokenized_text[-1] = new_token_id\n",
    "    tokenized_text.append(102)\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    optimizer = optim.Adam(encoder.parameters(), lr=1)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0.0001)\n",
    "    best = float(\"inf\")\n",
    "    for _ in tqdm(range(200)):\n",
    "        optimizer.zero_grad()\n",
    "        output = encoder({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        })[\"sentence_embedding\"]\n",
    "        \n",
    "        loss = criterion(output.squeeze(), target, torch.tensor(1.0))\n",
    "        loss.backward()\n",
    "        # set grad of non-new token to 0\n",
    "        # all ids except new_token_id\n",
    "        \n",
    "        encoder[0].auto_model.embeddings.word_embeddings.weight.grad[non_new_token_ids] = 0\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if loss.item() < best:\n",
    "            best = loss.item()\n",
    "    return best\n",
    "\n",
    "    \n",
    "target = torch.tensor(encoder.encode(text))\n",
    "soft_prompt_for_text(example_text, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: This\n",
      "encoded input: {'input_ids': tensor([[1212]]), 'attention_mask': tensor([[1]])}\n",
      "[' is', ',', '.']\n",
      "['This is', 'This,', 'This.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:20<00:00,  9.82it/s]\n",
      "100%|██████████| 200/200 [00:18<00:00, 10.73it/s]\n",
      "100%|██████████| 200/200 [00:19<00:00, 10.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: This,\n",
      "encoded input: {'input_ids': tensor([[1212,   11]]), 'attention_mask': tensor([[1, 1]])}\n",
      "[' of', ' however', ' in']\n",
      "['This, of', 'This, however', 'This, in']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:18<00:00, 10.54it/s]\n",
      "100%|██████████| 200/200 [00:20<00:00,  9.99it/s]\n",
      "100%|██████████| 200/200 [00:19<00:00, 10.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: This, in\n",
      "encoded input: {'input_ids': tensor([[1212,   11,  287]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "[' turn', ' my', ' the']\n",
      "['This, in turn', 'This, in my', 'This, in the']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:19<00:00, 10.18it/s]\n",
      "100%|██████████| 200/200 [00:21<00:00,  9.46it/s]\n",
      "100%|██████████| 200/200 [00:18<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: This.\n",
      "encoded input: {'input_ids': tensor([[1212,   13]]), 'attention_mask': tensor([[1, 1]])}\n",
      "[' Is', 'is', '\\n']\n",
      "['This. Is', 'This.is', 'This.\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:22<00:00,  9.02it/s]\n",
      "100%|██████████| 200/200 [00:22<00:00,  8.91it/s]\n",
      " 20%|██        | 40/200 [00:04<00:17,  9.28it/s]"
     ]
    }
   ],
   "source": [
    "from queue import PriorityQueue\n",
    "\n",
    "def dot_prod(text, target):\n",
    "    vec = encoder.encode(text, convert_to_tensor=True)\n",
    "    return vec @ target\n",
    "\n",
    "def iterative_soft_prompt(text, target, k=3, iter_num=100):\n",
    "    pq = PriorityQueue()\n",
    "    pq.put((0, text))\n",
    "    best_text = None\n",
    "    best_cossim = float('-inf')\n",
    "    for _ in range(iter_num):\n",
    "        text = pq.get()[1]\n",
    "        this_cossim = dot_prod(text, target)\n",
    "        if this_cossim > best_cossim:\n",
    "            best_cossim = this_cossim\n",
    "            best_text = text\n",
    "        print(\"text:\", text)\n",
    "        top_k_next_tokens = get_top_k_next_tokens(text, k)\n",
    "        print(top_k_next_tokens)\n",
    "        top_k_words = [text + token for token in top_k_next_tokens]\n",
    "        print(top_k_words)\n",
    "        scores = [(next_word, soft_prompt_for_text(next_word, target)) for next_word in top_k_words]\n",
    "        for next_word, score in scores:\n",
    "            pq.put((score, next_word))\n",
    "    return best_text, best_cossim\n",
    "        \n",
    "target = torch.tensor(encoder.encode(text))\n",
    "text, cossim = iterative_soft_prompt(\"This\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TMLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
