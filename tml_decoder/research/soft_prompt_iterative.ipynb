{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "text = \"\"\"Positional control of pneumatic manipulators for constructiobn tasks\n",
    "This paper describes solutions that can be applied to pneumatic manipulator\n",
    "\tproblems in positioning, both for angle trajectories and for long\n",
    "\tlinear trajectories, used in construction tasks. Optimal positioning of\n",
    "\ta pneumatic manipulator along angle trajectories with minimum control\n",
    "\tenergy consumption is given. The implementation of the control system\n",
    "\tis presented. Control algorithms for a long linear trajectory\n",
    "\tmanipulator based on two-phase and three-phase motion modes of the\n",
    "\tend-effector are investigated. Conventional and fuzzy logic controls of\n",
    "\ta pneumatic manipulator were applied and experimental testing was\n",
    "\tcarried out. The obtained results allow widening the application range\n",
    "\tof pneumatic manipulators in construction, particularly in gantry type\n",
    "\tmachines\"\"\"\n",
    "\n",
    "enc_tokenizer = encoder.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 50257])\n",
      "torch.Size([1, 9])\n",
      " chocolate\n",
      "next token in the sentence is:  chocolate\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "generator = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "example_text = \"I really like eating ice cream and drinking hot\"\n",
    "inputs = gpt_tokenizer(example_text, return_tensors=\"pt\")\n",
    "outputs = generator(**inputs, labels=inputs[\"input_ids\"])\n",
    "logits = outputs.logits\n",
    "print(logits.shape)\n",
    "am_tokens = torch.argmax(logits, dim=-1)\n",
    "print(am_tokens.shape)\n",
    "decoded_tokens = am_tokens[0].tolist()\n",
    "last_token = decoded_tokens[-1]\n",
    "decoded_token = gpt_tokenizer.decode(last_token)\n",
    "print(decoded_token)\n",
    "print(\"next token in the sentence is:\", decoded_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded input: {'input_ids': tensor([[  40, 1107,  588, 6600, 4771, 8566,  290, 7722, 3024]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[' chocolate', ' coffee', ' water']\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_top_k_next_tokens(text, k=1):\n",
    "    assert k > 0 and k <= len(gpt_tokenizer.vocab)\n",
    "    # Tokenize the input text\n",
    "    encoded_input = gpt_tokenizer(text, return_tensors='pt')\n",
    "    print(\"encoded input:\", encoded_input)\n",
    "    # Get model output\n",
    "    output = generator(**encoded_input)\n",
    "    logits = output.logits\n",
    "    probabilities = F.softmax(logits, dim=-1)[0][-1]\n",
    "\n",
    "    # Get top k indices for each position in the sequence\n",
    "    top_k_indices = torch.topk(probabilities, k, dim=-1).indices\n",
    "    # Decode each index using the tokenizer\n",
    "    decoded_tokens = [gpt_tokenizer.decode(int(idx)) for idx in top_k_indices]\n",
    "\n",
    "    return decoded_tokens\n",
    "\n",
    "# Specify the number of top-k tokens to retrieve\n",
    "k = 3\n",
    "result = get_top_k_next_tokens(example_text, k)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "<class 'torch.nn.modules.sparse.Embedding'>\n",
      "0.0599682554602623 2.869729995727539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54559/4090252663.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(p)\n"
     ]
    }
   ],
   "source": [
    "unused_token = next(\n",
    "        k for k, v in enc_tokenizer.vocab.items()\n",
    "        if \"[unused\" in k)\n",
    "unused_token_id = enc_tokenizer.vocab[unused_token]\n",
    "new_token_id = unused_token_id\n",
    "non_new_token_ids = torch.tensor([i for i in range(len(enc_tokenizer.vocab)) if i != new_token_id])\n",
    "\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "print(len(enc_tokenizer))\n",
    "print(type(encoder[0].auto_model.embeddings.word_embeddings))\n",
    "\n",
    "dot_prods = []\n",
    "\n",
    "for idx, p in enumerate(encoder[0].auto_model.embeddings.word_embeddings.parameters()):\n",
    "    p.requires_grad = True\n",
    "    t = torch.tensor(p)\n",
    "    for x in range(t.shape[0]):\n",
    "        subt = t[x]\n",
    "        dot_prods.append(torch.dot(subt, subt).item())\n",
    "\n",
    "print(min(dot_prods), max(dot_prods))\n",
    "\n",
    "assert any(p.requires_grad for p in encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:22<00:00,  9.00it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 11498.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn import CosineSimilarity\n",
    "\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "def soft_prompt_for_text(text, target):\n",
    "    tokenized_text = enc_tokenizer.encode(text)\n",
    "    tokenized_text[-1] = new_token_id\n",
    "    tokenized_text.append(102)\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    optimizer = optim.Adam(encoder.parameters(), lr=1)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0.0001)\n",
    "    best = float(\"inf\")\n",
    "    best_emb = None\n",
    "    for _ in tqdm(range(200)):\n",
    "        optimizer.zero_grad()\n",
    "        output = encoder({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        })[\"sentence_embedding\"]\n",
    "        \n",
    "        loss = criterion(output.squeeze(), target, torch.tensor(1.0))\n",
    "        loss.backward()\n",
    "        # set grad of non-new token to 0\n",
    "        # all ids except new_token_id\n",
    "        \n",
    "        encoder[0].auto_model.embeddings.word_embeddings.weight.grad[non_new_token_ids] = 0\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if loss.item() < best:\n",
    "            best = loss.item()\n",
    "            best_emb = encoder[0].auto_model.embeddings.word_embeddings.weight.data[new_token_id].cpu().numpy()\n",
    "    \n",
    "    # now discretization\n",
    "    cossims = []\n",
    "    for token in tqdm(range(len(enc_tokenizer.vocab))):\n",
    "        token_embedding = encoder[0].auto_model.embeddings.word_embeddings.weight.data[token].cpu().numpy()\n",
    "        cossims.append(CosineSimilarity()(torch.Tensor(token_embedding).unsqueeze(0), torch.Tensor(best_emb).unsqueeze(0)).item())\n",
    "    return -max(cossims)\n",
    "\n",
    "    \n",
    "target = torch.tensor(encoder.encode(text))\n",
    "soft_prompt_for_text(example_text, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos sim: This tensor(-0.0359)\n",
      "text: This\n",
      "encoded input: {'input_ids': tensor([[1212]]), 'attention_mask': tensor([[1]])}\n",
      "[' is', ',', '.']\n",
      "['This is', 'This,', 'This.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.18it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 11769.70it/s]\n",
      "100%|██████████| 200/200 [00:21<00:00,  9.50it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 11612.96it/s]\n",
      "100%|██████████| 200/200 [00:21<00:00,  9.23it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 11283.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-1.0, 'This is'), (-1.0, 'This,'), (-0.9999999403953552, 'This.')]\n",
      "cos sim: This is tensor(-0.0573)\n",
      "text: This is\n",
      "encoded input: {'input_ids': tensor([[1212,  318]]), 'attention_mask': tensor([[1, 1]])}\n",
      "[' a', ' the', ' not']\n",
      "['This is a', 'This is the', 'This is not']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.18it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 11884.70it/s]\n",
      "100%|██████████| 200/200 [00:21<00:00,  9.46it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 12454.96it/s]\n",
      "100%|██████████| 200/200 [00:19<00:00, 10.47it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 12343.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.9999998807907104, 'This is a'), (-1.0, 'This is the'), (-1.0, 'This is not')]\n",
      "cos sim: This is not tensor(-0.0120)\n",
      "text: This is not\n",
      "encoded input: {'input_ids': tensor([[1212,  318,  407]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "[' a', ' the', ' an']\n",
      "['This is not a', 'This is not the', 'This is not an']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:19<00:00, 10.41it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 12445.08it/s]\n",
      "100%|██████████| 200/200 [00:18<00:00, 10.61it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 12074.67it/s]\n",
      "100%|██████████| 200/200 [00:18<00:00, 10.72it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 12769.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-1.0, 'This is not a'), (-1.0000001192092896, 'This is not the'), (-1.0, 'This is not an')]\n",
      "cos sim: This is not the tensor(-0.0618)\n",
      "text: This is not the\n",
      "encoded input: {'input_ids': tensor([[1212,  318,  407,  262]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "[' first', ' end', ' case']\n",
      "['This is not the first', 'This is not the end', 'This is not the case']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:18<00:00, 10.72it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 12346.81it/s]\n",
      "100%|██████████| 200/200 [00:18<00:00, 10.82it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 12447.72it/s]\n",
      "100%|██████████| 200/200 [00:20<00:00,  9.98it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 12124.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.9999998211860657, 'This is not the first'), (-1.0, 'This is not the end'), (-1.0000001192092896, 'This is not the case')]\n",
      "cos sim: This is not the case tensor(-0.0988)\n",
      "text: This is not the case\n",
      "encoded input: {'input_ids': tensor([[1212,  318,  407,  262, 1339]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "['.', ' with', ' for']\n",
      "['This is not the case.', 'This is not the case with', 'This is not the case for']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:20<00:00,  9.83it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 12013.60it/s]\n",
      "100%|██████████| 200/200 [00:18<00:00, 10.59it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 12486.90it/s]\n",
      "100%|██████████| 200/200 [00:19<00:00, 10.41it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 11888.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.9999998211860657, 'This is not the case.'), (-1.0, 'This is not the case with'), (-1.0, 'This is not the case for')]\n",
      "cos sim: This is not a tensor(-0.0143)\n",
      "text: This is not a\n",
      "encoded input: {'input_ids': tensor([[1212,  318,  407,  257]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "[' good', ' new', ' problem']\n",
      "['This is not a good', 'This is not a new', 'This is not a problem']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:18<00:00, 10.72it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 12381.88it/s]\n",
      "100%|██████████| 200/200 [00:18<00:00, 10.72it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 12158.46it/s]\n",
      "100%|██████████| 200/200 [00:18<00:00, 10.69it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 12965.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.9999998807907104, 'This is not a good'), (-0.9999998211860657, 'This is not a new'), (-0.9999998807907104, 'This is not a problem')]\n",
      "cos sim: This is not an tensor(-0.0249)\n",
      "text: This is not an\n",
      "encoded input: {'input_ids': tensor([[1212,  318,  407,  281]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "[' easy', ' exhaustive', ' attempt']\n",
      "['This is not an easy', 'This is not an exhaustive', 'This is not an attempt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:18<00:00, 10.71it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 12437.04it/s]\n",
      "100%|██████████| 200/200 [00:18<00:00, 10.73it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 12495.07it/s]\n",
      "100%|██████████| 200/200 [00:19<00:00, 10.23it/s]\n",
      "100%|██████████| 30522/30522 [00:02<00:00, 11054.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-1.0, 'This is not an easy'), (-1.0000001192092896, 'This is not an exhaustive'), (-0.9999999403953552, 'This is not an attempt')]\n",
      "cos sim: This is not an exhaustive tensor(0.0294)\n",
      "text: This is not an exhaustive\n",
      "encoded input: {'input_ids': tensor([[ 1212,   318,   407,   281, 36049]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "[' list', ' review', ' study']\n",
      "['This is not an exhaustive list', 'This is not an exhaustive review', 'This is not an exhaustive study']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 49/200 [00:05<00:15,  9.81it/s]"
     ]
    }
   ],
   "source": [
    "from queue import PriorityQueue\n",
    "\n",
    "def dot_prod(text, target):\n",
    "    vec = encoder.encode(text, convert_to_tensor=True)\n",
    "    return vec @ target\n",
    "\n",
    "def iterative_soft_prompt(text, target, k=3, iter_num=100):\n",
    "    pq = PriorityQueue()\n",
    "    pq.put((0, text))\n",
    "    best_text = None\n",
    "    best_cossim = float('-inf')\n",
    "    for _ in range(iter_num):\n",
    "        text = pq.get()[1]\n",
    "        this_cossim = dot_prod(text, target)\n",
    "        print(\"cos sim:\", text, this_cossim)\n",
    "        if this_cossim > best_cossim:\n",
    "            best_cossim = this_cossim\n",
    "            best_text = text\n",
    "        print(\"text:\", text)\n",
    "        top_k_next_tokens = get_top_k_next_tokens(text, k)\n",
    "        print(top_k_next_tokens)\n",
    "        top_k_words = [text + token for token in top_k_next_tokens]\n",
    "        print(top_k_words)\n",
    "        soft_prompt_scores = [(soft_prompt_for_text(next_word, target), next_word) for next_word in top_k_words]\n",
    "        print(soft_prompt_scores)\n",
    "        for sp_score, next_word in soft_prompt_scores:\n",
    "            pq.put((sp_score, next_word))\n",
    "    return best_text, best_cossim\n",
    "        \n",
    "target = torch.tensor(encoder.encode(text))\n",
    "text, cossim = iterative_soft_prompt(\"This\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TMLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
