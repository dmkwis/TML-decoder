# Unveiling the Semantic Essence: Text Embeddings Summarization

## Introduction

The project delves into the problem of generating concise and accurate descriptions for clusters of documents represented as text embeddings. Text embeddings are high-dimensional vector representations of text that capture semantic meaning. By summarizing these clusters effectively, we aim to unlock valuable insights from large text collections, enhance automated summarization capabilities, and improve machine understanding of language.

## Problem Definition

Given a set of documents ${D_1, D_2, ..., D_n}$, each represented as a vector $v_i$ in a high-dimensional space, and known clusters ${C_1, C_2, ..., C_k}$ of documents, the objective is to generate a description $S_j$ for each cluster $C_j$. The generated description should maximize the cosine similarity between its vector representation ($s_j$) and the centroid of the cluster ($Î¼_ j$).

## Aplications and Significance

The ability to generate meaningful summaries of text embedding clusters has several important applications:

1. **Vector Databases:** Quickly understanding the thematic essence of large text collections without needing to access the original documents.
2. **Automated Summarization at Scale:** Contributing to the field of automated text summarization by providing a scalable method to condense and interpret vast amounts of text data.
3. **Enhancing Machine Understanding:** Aiding in the debugging of encoder models that are widely used in natural language processing tasks.

## New Dataset

To tackle this problem, a new dataset was created consisting of:

- 2200 abstracts with 20,000 keywords sourced from the paper [_Improved automatic keyword extraction given more linguistic knowledge_](https://aclanthology.org/W03-1028.pdf).
- 2166 clusters containing at least 2 documents each.
- Summaries of documents tagged with specific keywords, generated using GPT-4o. These summaries serve as a benchmark for evaluating the performance of our methods.

### Dataset Creation

The dataset was created by extracting abstracts and keywords from the paper. The abstracts were then clustered based on the keywords. Each cluster was assigned a summary generated by GPT-4.

For clustering, we conducted several experiments. Ultimately, we found that clustering the documents using single keywords yielded the best results. Using pairs or triples of keywords resulted in clusters that were too small, leading to a loss in the size of the dataset. We filtered out clusters with less than 2 documents to ensure that the clusters were large enough to be meaningful.

### Dive into the Dataset

The dataset consists of 2166 clusters, with each cluster containing at least 2 documents. The clusters are tagged with keywords, and each cluster has a summary generated by GPT-4o. Here are a few examples of clusters from the dataset:

1. **Cluster 1:**
   - Keyword: `Academic Libraries`
   - Summary: _"Academic libraries' evolving roles"_
   - Documents: _"An empirical investigation of the influences of the degree of interactivity on user outcomes in a multimedia environment"_, _"The disconnect continues: Digital content providers"_, _"Bistability of harmonically forced relaxation oscillations"_
2. **Cluster 2:**
   - Keyword: `Colour Graphics`
   - Summary: _"Computational advancements in colour graphics."_
   - Documents: _"Physical quantum algorithms"_, _"Separation and tracking of multiple broadband sources with one electromagnetic vector sensor"_, _"Relevance of Web documents: Ghosts consensus method"_
3. **Cluster 3:**
   - Keyword: `Artificial Life`
   - Summary: _"Artificial life insights and innovations."_
   - Documents: _"Critical lines identification on voltage collapse analysis"_, _"The semi-algebraic theory of stochastic games"_, _"The Information Age interview: Capital One"_
4. **Cluster 4:**
   - Keyword: `E-government`
   - Summary: _"E-government: Optimization, Network Reconstruction, Crisis Management."_
   - Documents: _"A method of determining a sequence of the best solutions to the problems of optimization on finite sets and the problem of network reconstruction"_, _"Security crisis management: The basics"_

However, there are some clusters that are not as well-defined. There are not many of them, but they are still present in the dataset. It appears it's due to the nature of the dataset, where some clusters are more ambiguous or contain documents that are not closely related to the keyword. This poses a challenge for the summarization task, as the model needs to generate a coherent summary for such clusters. Here is example:

- Keyword: `software applications`
- Summary: _"Optimizing design workflows."_
- Documents: _"A simple graphic approach for observer decomposition"_, _"Five axis NC milling of ruled surfaces optimal geometry of a conical tool"_

## Proposed Methodology

### Hard Prompting

This approach involves adding initial prompts to guide the language model at the beginning of the generation process. Prompts like _"These documents describe..."_ or _"Overview of this topic is..."_ are used to steer the model towards producing relevant summaries.

### Soft Prompting

Soft prompting, inspired by the work of Liu et al. (2022), involves two techniques:

1. **Choosing new nodes with soft prompts:** Instead of randomly selecting nodes during the generation process, soft prompts are used to guide the selection, potentially leading to more coherent and relevant summaries.
2. **Creating new nodes with soft prompts:** To address the issue of language models generating words based on probability distributions that may not align with the target domain, soft prompts are used to create new states that are closer to the cluster's average embedding. This helps ensure that the generated summaries are more semantically aligned with the cluster's content.

## Results and Evaluation

<!-- TODO -->

Tu jest jeszcze pusto, bo nie mamy wszystkiego.
